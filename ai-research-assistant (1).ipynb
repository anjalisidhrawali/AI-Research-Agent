{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction","metadata":{}},{"cell_type":"markdown","source":"Keeping up with the rapid growth of scientific publications is a major challenge for researchers. Traditional literature reviews are time-consuming, manual, and difficult to scale. To address this, we built an AI Research Assistant powered by Groq LLaMA models and the arXiv API.\n\nThe system automates the entire pipeline of a literature review:\n\nüîç Search relevant papers from arXiv.\nüìÑ Extract & summarize content from PDFs.\n‚ú® Generate concise summaries for quick understanding.\nüîÑ Iteratively suggest new research directions, enabling continuous exploration.\n\nThis project showcases how agentic AI systems can go beyond simple question answering to perform autonomous academic exploration, assisting researchers, students, and professionals in navigating vast research landscapes.","metadata":{}},{"cell_type":"markdown","source":"# Installing Dependencies","metadata":{}},{"cell_type":"code","source":"!pip install -q groq arxiv PyPDF2 termcolor requests tenacity","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:11:28.990013Z","iopub.execute_input":"2025-08-28T05:11:28.990264Z","iopub.status.idle":"2025-08-28T05:11:37.086189Z","shell.execute_reply.started":"2025-08-28T05:11:28.990236Z","shell.execute_reply":"2025-08-28T05:11:37.085309Z"}},"outputs":[{"name":"stdout","text":"  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m131.4/131.4 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Importing Necessary Libraries","metadata":{}},{"cell_type":"code","source":"import os\nimport re\nimport time\nimport json\nimport requests\nfrom dataclasses import dataclass\nfrom typing import List, Tuple, Optional\n\nimport arxiv\nfrom PyPDF2 import PdfReader\nfrom termcolor import colored\nfrom tenacity import retry, wait_exponential_jitter, stop_after_attempt, retry_if_exception_type\nfrom groq import Groq\nfrom groq._exceptions import RateLimitError, APIStatusError\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\ngroq_api_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n\nos.environ[\"GROQ_API_KEY\"] = groq_api_key\n\nGROQ_MODEL = \"llama-3.1-70b-versatile\"\n\n# Pipeline parameters\nMAX_RESULTS = 10\nNUMBER_OF_TURNS = 10\nINITIAL_SEARCH_TERM = \"coding ability of large language models\"\nPAPERS_DIR = \"research_papers\"\n\n# Chunking ‚Äî character-based\nCHUNK_SIZE = 6000\nCHUNK_OVERLAP = 400       \nMAX_CHUNKS = 30\n\n# Output controls\nMAX_SUMMARY_TOKENS = 512 \nTEMPERATURE = 0.3         \n\n# Basic checks\nif not groq_api_key:\n    print(colored(\"‚ö†Ô∏è  Missing GROQ_API_KEY. Set it in your environment or this cell.\", \"yellow\"))\nelse:\n    print(colored(\"‚úì GROQ API key detected\", \"green\"))\nprint(colored(f\"Using Groq model: {GROQ_MODEL}\", \"cyan\"))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:11:40.788646Z","iopub.execute_input":"2025-08-28T05:11:40.788939Z","iopub.status.idle":"2025-08-28T05:11:42.155615Z","shell.execute_reply.started":"2025-08-28T05:11:40.788908Z","shell.execute_reply":"2025-08-28T05:11:42.154859Z"}},"outputs":[{"name":"stdout","text":"‚úì GROQ API key detected\nUsing Groq model: llama-3.1-70b-versatile\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Helper Function","metadata":{}},{"cell_type":"code","source":"def sanitize_folder_name(name: str, max_length: int = 60) -> str:\n    s = re.sub(r'[\\\\/*?:\"<>|]', \"_\", name).strip()\n    s = re.sub(r\"\\s+\", \"_\", s)\n    if len(s) > max_length:\n        s = s[:max_length].rsplit(\"_\", 1)[0]\n    return s or \"untitled\"\n\ndef ensure_dir(path: str) -> str:\n    os.makedirs(path, exist_ok=True)\n    return path\n\ndef chunk_text(text: str, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP, max_chunks: int = MAX_CHUNKS) -> List[str]:\n    if not text:\n        return []\n    chunks = []\n    i = 0\n    n = len(text)\n    while i < n and len(chunks) < max_chunks:\n        end = min(i + chunk_size, n)\n        chunks.append(text[i:end])\n        if end == n:\n            break\n        i = end - overlap\n        if i < 0: i = 0\n    return chunks\n\ndef save_text(path: str, content: str):\n    ensure_dir(os.path.dirname(path))\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        f.write(content)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:11:48.954910Z","iopub.execute_input":"2025-08-28T05:11:48.955457Z","iopub.status.idle":"2025-08-28T05:11:48.963840Z","shell.execute_reply.started":"2025-08-28T05:11:48.955430Z","shell.execute_reply":"2025-08-28T05:11:48.963038Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# ArXiv tools (Client API)","metadata":{}},{"cell_type":"code","source":"def get_arxiv_papers(query: str, max_results: int = 5):\n    try:\n        client = arxiv.Client(page_size=max_results)\n        search = arxiv.Search(query=query, max_results=max_results, sort_by=arxiv.SortCriterion.Relevance)\n        return list(client.results(search))\n    except Exception as e:\n        print(f\"Error: {e}\")\n        return []\n\ndef download_pdf(url: str, filename: str, folder: str) -> str:\n    os.makedirs(folder, exist_ok=True)\n    path = os.path.join(folder, filename)\n    r = requests.get(url, timeout=60)\n    r.raise_for_status()\n    with open(path, \"wb\") as f:\n        f.write(r.content)\n    return path\n\ndef extract_text_from_pdf(pdf_path: str) -> str:\n    reader = PdfReader(pdf_path)\n    return \"\\n\".join([page.extract_text() or \"\" for page in reader.pages]).strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:11:59.332147Z","iopub.execute_input":"2025-08-28T05:11:59.332877Z","iopub.status.idle":"2025-08-28T05:11:59.339657Z","shell.execute_reply.started":"2025-08-28T05:11:59.332850Z","shell.execute_reply":"2025-08-28T05:11:59.338622Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"# Groq (LLaMA) Chat Helpers","metadata":{}},{"cell_type":"code","source":"client = None\nif groq_api_key:\n    client = Groq(api_key=groq_api_key)\n\n@retry(\n    wait=wait_exponential_jitter(initial=1, max=20),\n    stop=stop_after_attempt(5),\n    retry=retry_if_exception_type((RateLimitError, APIStatusError, requests.exceptions.RequestException))\n)\ndef llama_chat(messages, model=GROQ_MODEL, temperature=TEMPERATURE, max_tokens=MAX_SUMMARY_TOKENS):\n    if client is None:\n        raise RuntimeError(\"Groq client not initialized (missing key).\")\n    resp = client.chat.completions.create(\n        model=model, messages=messages, temperature=temperature, max_tokens=max_tokens\n    )\n    return resp.choices[0].message.content.strip()\n\ndef choose_paper_with_llama(papers) -> Tuple[Optional[int], str]:\n    listing = [f\"{i}. {p.title.strip()}\\nAbstract: {(p.summary or '').strip()}\" for i, p in enumerate(papers)]\n    prompt = (\n        \"You are a careful academic assistant. From the numbered list of papers, \"\n        \"pick the SINGLE best paper for the search term. \"\n        \"Respond in exactly this JSON format:\\n\"\n        '{ \"choice\": <number>, \"reason\": \"<brief reason without numeric lists>\" }\\n\\n'\n        \"Papers:\\n\" + \"\\n\".join(listing)\n    )\n    messages = [\n        {\"role\": \"system\", \"content\": \"You select the most promising academic paper and explain briefly.\"},\n        {\"role\": \"user\", \"content\": prompt},\n    ]\n    try:\n        out = llama_chat(messages, max_tokens=256)\n        m = re.search(r'\\{.*\\}', out, re.S)\n        if m:\n            j = json.loads(m.group(0))\n            return int(j.get(\"choice\")), j.get(\"reason\", \"\")\n        return (int(re.search(r'(\\d+)', out).group(1)) if re.search(r'(\\d+)', out) else None), out\n    except Exception:\n        return None, \"\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:12:09.585361Z","iopub.execute_input":"2025-08-28T05:12:09.585985Z","iopub.status.idle":"2025-08-28T05:12:09.946118Z","shell.execute_reply.started":"2025-08-28T05:12:09.585963Z","shell.execute_reply":"2025-08-28T05:12:09.945288Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Summarization Helpers","metadata":{}},{"cell_type":"code","source":"def safe_summarize_text(text: str):\n    chunks = chunk_text(text, CHUNK_SIZE, CHUNK_OVERLAP)\n    summaries = []\n\n    for i, chunk in enumerate(chunks, 1):\n        if not chunk.strip():\n            print(f\"‚ö†Ô∏è Skipping empty chunk {i}\")\n            continue\n        try:\n            summary = groq_generate(\n                f\"Summarize the following paper content:\\n\\n{chunk}\",\n                max_tokens=MAX_SUMMARY_TOKENS,\n                temperature=TEMPERATURE,\n            )\n            if summary and summary.strip():\n                summaries.append(summary.strip())\n        except Exception as e:\n            print(f\"‚úó Summarization failed on chunk {i}: {e}\")\n\n    return \"\\n\\n\".join(summaries) if summaries else None\n\ndef search_arxiv(query, max_results=10): \n    try: \n       search = arxiv.Search(\n         query=query,\n         max_results=max_results,\n         sort_by=arxiv.SortCriterion.Relevance \n       ) \n       return list(search.results()) \n    except Exception as e: \n       print(f\"‚úó Arxiv search failed: {e}\") \n       return []\n\ndef groq_generate(prompt, max_tokens=200, temperature=0.7):\n    if not prompt or not prompt.strip():\n        print(\"‚ö†Ô∏è Empty prompt given to Groq, skipping.\")\n        return None\n    try:\n        client = Groq(api_key=groq_api_key)\n        resp = client.chat.completions.create(\n            model=\"llama-3.1-8b-instant\",\n            messages=[{\"role\": \"user\", \"content\": prompt}],\n            max_tokens=max_tokens,\n            temperature=temperature\n        ) \n        return resp.choices[0].message.content.strip()\n    except Exception as e: \n        print(f\"‚úó Groq call failed: {e}\")\n        return None\n\ndef extract_paper_text(paper, folder=PAPERS_DIR):\n    try:\n        pdf_path = os.path.join(folder, f\"{paper.get_short_id()}.pdf\")\n        paper.download_pdf(filename=pdf_path)\n        return f\"Title: {paper.title.strip()}\\n\\nAbstract: {paper.summary.strip()}\"\n    except Exception as e:\n        print(f\"‚úó Failed to extract text: {e}\")\n        return \"\"\n\ndef save_summary(paper, summary, folder=PAPERS_DIR):\n    try:\n        fname = os.path.join(folder, f\"{paper.get_short_id()}_summary.txt\")\n        os.makedirs(folder, exist_ok=True)\n        with open(fname, \"w\", encoding=\"utf-8\") as f:\n            f.write(summary)\n        print(f\"‚úì Saved summary to {fname}\")\n    except Exception as e:\n        print(f\"‚úó Failed to save summary: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:28:49.453834Z","iopub.execute_input":"2025-08-28T05:28:49.454170Z","iopub.status.idle":"2025-08-28T05:28:49.465862Z","shell.execute_reply.started":"2025-08-28T05:28:49.454145Z","shell.execute_reply":"2025-08-28T05:28:49.464905Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# Iterative Paper Pipeline","metadata":{}},{"cell_type":"code","source":"def run_iteration(search_term, prev_paper_title=\"\"):\n    print(colored(f\"Searching arXiv for: {search_term}\", \"green\"))\n    papers = search_arxiv(search_term)\n\n    if not papers:\n        print(colored(\"‚ö†Ô∏è No papers found. Keeping same search term.\", \"yellow\"))\n        return search_term, None\n\n    chosen_paper = papers[0]  # take top result\n    print(colored(f\"Chosen paper: {chosen_paper.title}\", \"magenta\"))\n\n    text = extract_paper_text(chosen_paper)\n    if text:\n        final_summary = safe_summarize_text(text)\n        if final_summary:\n            save_summary(chosen_paper, final_summary)\n        \n# --- KEY CHANGE: force Groq to output only a topic ---\n    new_prompt = (\n        f\"Based on the paper '{chosen_paper.title}', suggest ONE concise related \"\n        f\"research topic (5-8 words max, no punctuation).\"\n    )\n    new_term = groq_generate(new_prompt, max_tokens=30)\n\n    if not new_term or len(new_term.split()) < 2:\n        print(colored(\"‚ö†Ô∏è Invalid search term generated, reusing old term.\", \"yellow\"))\n        new_term = search_term\n\n    return new_term, chosen_paper\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:28:55.040075Z","iopub.execute_input":"2025-08-28T05:28:55.040374Z","iopub.status.idle":"2025-08-28T05:28:55.046907Z","shell.execute_reply.started":"2025-08-28T05:28:55.040353Z","shell.execute_reply":"2025-08-28T05:28:55.046095Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# Main Loop","metadata":{}},{"cell_type":"code","source":"print(colored(\"=== LLaMA Research Assistant (Groq) ===\", \"cyan\"))\nif not groq_api_key:\n    raise SystemExit(\"Please set GROQ_API_KEY and re-run.\")\n\nensure_dir(PAPERS_DIR)\n\nchosen_paper = None\nsearch_term = INITIAL_SEARCH_TERM or \"artificial intelligence research\"\n\nfor turn in range(1, NUMBER_OF_TURNS + 1):\n    print(colored(f\"\\n=== Research Turn {turn}/{NUMBER_OF_TURNS} ===\", \"cyan\"))\n\n    prev_title = chosen_paper.title if chosen_paper else \"No previous paper\"\n\n    search_term, chosen_paper = run_iteration(\n        search_term,\n        prev_paper_title=prev_title\n    )\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-28T05:29:00.221078Z","iopub.execute_input":"2025-08-28T05:29:00.221627Z","iopub.status.idle":"2025-08-28T05:30:49.525912Z","shell.execute_reply.started":"2025-08-28T05:29:00.221597Z","shell.execute_reply":"2025-08-28T05:30:49.525140Z"}},"outputs":[{"name":"stdout","text":"=== LLaMA Research Assistant (Groq) ===\n\n=== Research Turn 1/10 ===\nSearching arXiv for: coding ability of large language models\nChosen paper: Testing the Effect of Code Documentation on Large Language Model Code Understanding\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2345965408.py:29: DeprecationWarning: The 'Search.results' method is deprecated, use 'Client.results' instead\n  return list(search.results())\n","output_type":"stream"},{"name":"stdout","text":"‚úì Saved summary to research_papers/2404.03114v1_summary.txt\n\n=== Research Turn 2/10 ===\nSearching arXiv for: Assessing the Impact of Code Comments on Debugging.\nChosen paper: Out-Of-Place debugging: a debugging architecture to reduce debugging interference\n‚úì Saved summary to research_papers/1811.02034v1_summary.txt\n\n=== Research Turn 3/10 ===\nSearching arXiv for: Reducing Debugging Interference in Cloud Computing Environments\nChosen paper: Out-Of-Place debugging: a debugging architecture to reduce debugging interference\n‚úì Saved summary to research_papers/1811.02034v1_summary.txt\n\n=== Research Turn 4/10 ===\nSearching arXiv for: Investigating Real-Time Debugging Techniques for High-Performance Systems\nChosen paper: Out-Of-Place debugging: a debugging architecture to reduce debugging interference\n‚úì Saved summary to research_papers/1811.02034v1_summary.txt\n\n=== Research Turn 5/10 ===\nSearching arXiv for: Improving debugging techniques for distributed and concurrent systems.\nChosen paper: Automated Dynamic Concurrency Analysis for Go\n‚úì Saved summary to research_papers/2105.11064v1_summary.txt\n\n=== Research Turn 6/10 ===\nSearching arXiv for: Analyzing Concurrency Bugs in Parallel Functional Programming Languages\nChosen paper: Automated Dynamic Concurrency Analysis for Go\n‚úì Saved summary to research_papers/2105.11064v1_summary.txt\n\n=== Research Turn 7/10 ===\nSearching arXiv for: Analyzing Concurrency Bugs in Complex Distributed Systems.\nChosen paper: Towards Extending the Range of Bugs That Automated Program Repair Can Handle\n‚úì Saved summary to research_papers/2211.03911v1_summary.txt\n\n=== Research Turn 8/10 ===\nSearching arXiv for: Improving Automated Program Repair for Complex Defects\nChosen paper: Can defects be fixed with weak test suites? An analysis of 50 defects from Defects4J\n‚úì Saved summary to research_papers/1705.04149v2_summary.txt\n\n=== Research Turn 9/10 ===\nSearching arXiv for: Evaluating the effectiveness of weak test suites fixes.\nChosen paper: Suggestions on Test Suite Improvements with Automatic Infection and Propagation Analysis\n‚úì Saved summary to research_papers/1909.04770v1_summary.txt\n\n=== Research Turn 10/10 ===\nSearching arXiv for: Analyzing Propagation of Faults in Open Source Code\nChosen paper: ARMORY: Fully Automated and Exhaustive Fault Simulation on ARM-M Binaries\n‚úì Saved summary to research_papers/2105.13769v1_summary.txt\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"# Conclusion","metadata":{}},{"cell_type":"markdown","source":"This project demonstrates how LLMs (LLaMA via Groq API) can be combined with arXiv search, automated summarization, and iterative refinement to build an autonomous AI Research Assistant. By integrating paper retrieval, PDF parsing, text chunking, summarization, and topic expansion into a single pipeline, the system can:\n\n* Continuously explore new research directions.\n* Generate concise, human-readable summaries of academic papers.\n* Suggest fresh and relevant research topics for further investigation.\n\nThe assistant not only streamlines the literature review process but also highlights the potential of agentic AI systems that learn, adapt, and build knowledge iteratively.\n\nWhile the current version focuses on arXiv papers, the pipeline can be extended to other sources (PubMed, Semantic Scholar, IEEE Xplore) and enhanced with advanced summarization techniques, semantic ranking, and interactive interfaces (voice/chat apps).\n\nIn short, this project is a step toward creating autonomous research companions that augment human intelligence in navigating the ever-growing landscape of scientific knowledge.","metadata":{}},{"cell_type":"markdown","source":"# THANK YOU","metadata":{}}]}